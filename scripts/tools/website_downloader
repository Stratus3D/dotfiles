#!/usr/bin/env bash

# Unoffical Bash "strict mode"
# http://redsymbol.net/articles/unofficial-bash-strict-mode/
set -euo pipefail
IFS=$'\t\n' # Stricter IFS settings
#ORIGINAL_IFS=$IFS

usage() {
    cat <<EOF
    Usage: website_downloader [command]

    Commands:
    site [url] [destination dir]         Download all pages on the domain
    subdirectory [url] [destination dir] Download URL and sub pages
    single_page [url] [destination dir]  Download a single URL

    Options:
    --no-localize                        Don't localize links. With this option
                                         URLs may not work on your local copy
EOF
}

error_exit() {
  printf "Error: %s\\n\\n" "$@"
  usage
  exit 1
}

get_value() {
  raw_flag=$1
  first=${1#*'='}
  second=$2

  # If the raw flag contains an equals sign and the value after it (first) is
  # not null return first, otherwise return the next argument (second)
  if [[ "$raw_flag" == *=* ]] && [ -n "$first" ]; then
    echo "$first"
  else
    if [[ "$second" == -* ]]; then
      echo "$second"
    else
      echo "$second"
    fi
  fi
}

get_shift_count() {
  if [[ "$1" == *=* ]]; then
    echo "1"
  else
    echo "2"
  fi
}

download() {
  url=$1;
  #domain=?
  cookies_file=~/cookies.txt

  # Follow links and download the whole site
  wget "$url" --recursive \
      # Don't overwrite files when downloading
    --no-clobber \
    # Download CSS and other assets
    --page-requisites \
    # Save files with the .html extension
    --html-extension \
    # Change URLs in links so they work offline
    --convert-links \
    # modify filenames so they will work in Windows
    --restrict-file-names=windows \
    # Limit the download to this directory
    --domains $domain \
    # Don't follow links outside the directory
    --no-parent "$url" \
    --mirror \
    # Save HTML and CSS with the proper extensions
    --adjust-extension \
    -e robots=off \
    --wait=9 --limit-rate=10K \
    # Use cookies from file. Usefully when dealing with site behind a login page
    #--load-cookies=$cookies_file
}

url=${1:-}
directory=${2:-}

if [ -z "$url" ]; then
  error_exit "URL must be provided as first argument"
fi

if [ -z "$directory" ]; then
  error_exit "Output directory must be provided as second argument"
fi

if [ $# -gt 0 ]; then
    lower_command=$(echo "$1" | awk '{print tolower($0)}');

    case $lower_command in
        'site')
            if [ $# -gt 2 ]; then
                download "$url" "$directory"
            else
                echo "You must provide a URL and a destination directory"
            fi;;
        'subdirectory')
            print_fixmes;;
        'single_page')
            print_optimizes;;
        *)
            usage;;
    esac
else
    usage;
fi
