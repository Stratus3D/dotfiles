#!/usr/bin/env bash

# Unoffical Bash "strict mode"
# http://redsymbol.net/articles/unofficial-bash-strict-mode/
set -euo pipefail
IFS=$'\t\n' # Stricter IFS settings
#ORIGINAL_IFS=$IFS

usage() {
    cat <<EOF
    Usage: website_downloader [command]

    Commands:
    site [url] [destination dir]         Download all pages on the domain
    subdirectory [url] [destination dir] Download URL and sub pages
    single_page [url] [destination dir]  Download a single URL

    Options:
    --no-localize                        Don't localize links. With this option
                                         URLs may not work on your local copy
EOF
}

error_exit() {
  printf "Error: %s\\n\\n" "$@"
  usage
  exit 1
}

get_value() {
  raw_flag=$1
  first=${1#*'='}
  second=$2

  # If the raw flag contains an equals sign and the value after it (first) is
  # not null return first, otherwise return the next argument (second)
  if [[ "$raw_flag" == *=* ]] && [ -n "$first" ]; then
    echo "$first"
  else
    if [[ "$second" == -* ]]; then
      echo "$second"
    else
      echo "$second"
    fi
  fi
}

get_shift_count() {
  if [[ "$1" == *=* ]]; then
    echo "1"
  else
    echo "2"
  fi
}

download() {
  url=$1;
  #domain=?
  cookies_file=~/cookies.txt

  # Follow links and download the whole site
  wget "$url" --recursive \
      # Don't overwrite files when downloading
    --no-clobber \
    # Download CSS and other assets
    --page-requisites \
    # Save files with the .html extension
    --html-extension \
    # Change URLs in links so they work offline
    --convert-links \
    # modify filenames so they will work in Windows
    --restrict-file-names=windows \
    # Limit the download to this directory
    --domains $domain \
    # Don't follow links outside the directory
    --no-parent "$url" \
    --mirror \
    # Save HTML and CSS with the proper extensions
    --adjust-extension \
    -e robots=off \
    --wait=9 --limit-rate=10K \
    # Use cookies from file. Usefully when dealing with site behind a login page
    #--load-cookies=$cookies_file
}

url=${1:-}
directory=${2:-}

if [ -z "$url" ]; then
  error_exit "URL must be provided as first argument"
fi

if [ -z "$directory" ]; then
  error_exit "Output directory must be provided as second argument"
fi

# Base command
command_array=("$url" --recursive)

shift 2

while :; do
    case ${1:-} in
        -h|-\?|--help)
            usage
            exit
            ;;
        -r|--path-regex|--path-regex=*)
            path_regex=$(get_value "$1" "${2:-}")
            shift $(get_shift_count "$1")

            # Add path regex flags
            command_array+=()
            ;;
        -n|--no-overwrite)
            shift

            # Add flags to tell wget not to overwrite existing files
            command_array+=(--no-clobber)
            ;;
        -s|--rate-limit|--rate-limit=*)
            rate=$(get_value "$1" "${2:-}")
            shift $(get_shift_count "$1")

            # Add flags to tell wget to rate limit requests
            command_array+=(--wait="$rate" '--limit-rate=10K')
            ;;
        -l|--fully-local)
            shift

            # Add flags to tell wget to format pages for local viewing
            # (with localized links and downloaded assets)
            command_array+=(--page-requisites --html-extension
            --convert-links '--restrict-file-names=windows' --mirror --adjust-extension)
            ;;
        -d|--only-subdirectory)
            shift

            command_array+=(--no-parent "$url")
            ;;
        --)
            shift
            break
            ;;
        *)
            if [ -z "${1:-}" ]; then
                break
            else
                echo "Unknown option ${1:-}"
                error_exit
            fi
    esac
done

set -x
wget "${command_array[@]}"
set +x
